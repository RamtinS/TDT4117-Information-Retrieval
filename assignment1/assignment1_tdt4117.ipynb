{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00a8eb",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 1: Boolean Model, TF-IDF, and Data Retrieval vs. Information Retrieval Conceptual Questions\n",
    "\n",
    "**Student names**: Ramtin Forouzandehjoo Samavat <br>\n",
    "**Group number**: 30 <br>\n",
    "**Date**: 09.09.2025\n",
    "\n",
    "## Important notes\n",
    "Please carefully read the following notes and consider them for the assignment delivery. Submissions that do not fulfill these requirements will not be assessed and should be submitted again.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. The assignment must be delivered in ipynb format.\n",
    "3. The assignment must be typed. Handwritten assignments are not accepted.\n",
    "\n",
    "**Due date**: 14.09.2025 23:59\n",
    "\n",
    "In this assignment, you will:\n",
    "- Implement a Boolean retrieval model\n",
    "- Compute TF-IDF vectors for documents\n",
    "- Run retrieval on queries\n",
    "- Answer conceptual questions \n",
    "\n",
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready — just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3249058",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "773d293f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:07:07.024043Z",
     "start_time": "2025-09-09T09:07:07.010559Z"
    }
   },
   "source": [
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {} # Dictionary with documents.\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "documents = parse_cranfield(CRAN_PATH)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "63b8f900",
   "metadata": {},
   "source": [
    "## 1.1 – Boolean Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81516f89",
   "metadata": {},
   "source": [
    "### 1.1.1 Tokenize documents\n",
    "\n",
    "Implement tokenization using the given list of stopwords. Create a list of normalized terms per document (e.g., lowercase, remove punctuation/digits; drop stopwords). Store the token lists to use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "id": "d78a135a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:16:59.509329Z",
     "start_time": "2025-09-09T09:16:59.475436Z"
    }
   },
   "source": [
    "# TODO: Implement tokenization using the given list of stopwords, create list of terms per document\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "# Your code here\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize(text, stopwords):\n",
    "  tokens = re.findall(r\"\\b[a-z]+\\b\", text.lower()) # Convert to lowercase and remove all non-alphabetic characters.\n",
    "  tokens = [word for word in tokens if word not in stopwords] # Only keep words that are not stopwords.\n",
    "  return tokens\n",
    "\n",
    "document_tokens = {} # Dictionary<doc_id, token> to store each token.\n",
    "for doc_id, doc in documents.items():\n",
    "  combined_text = f\"{doc['title']} {doc['abstract']}\"\n",
    "  document_tokens[doc_id] = tokenize(combined_text, STOPWORDS)\n",
    "\n",
    "# Check if done correctly.\n",
    "for i in range(1, 3):\n",
    "  print(f\"Doc {i}: {document_tokens[i][:30]} ...\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: ['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free'] ...\n",
      "Doc 2: ['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'study', 'high', 'speed', 'viscous', 'flow', 'past', 'two', 'dimensional', 'body', 'usually'] ...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "3183f40d",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "\n",
    "Create a set (or list) of unique terms from all tokenized documents. Report the number of unique terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa9cc192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:17:01.321480Z",
     "start_time": "2025-09-09T09:17:01.312300Z"
    }
   },
   "source": [
    "# TODO: Create a set or list of unique terms\n",
    "\n",
    "# Report: \n",
    "# - Number of unique terms\n",
    "\n",
    "# Your code here\n",
    "\n",
    "vocabulary = set() # Will only keep unique tokens.\n",
    "for tokens in document_tokens.values():\n",
    "  vocabulary.update(tokens) # Add each unique term from each token to the vocabulary.\n",
    "\n",
    "print(f\"Number of unique terms in the vocabulary: {len(vocabulary)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms in the vocabulary: 6928\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "4eb02912",
   "metadata": {},
   "source": [
    "### Build inverted index\n",
    "\n",
    "For each term, store the list (or set) of document IDs where the term appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "393b2683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:17:25.143235Z",
     "start_time": "2025-09-09T09:17:25.120774Z"
    }
   },
   "source": [
    "\n",
    "# TODO: For each term, store list of document IDs where the term appears\n",
    "# Your code here\n",
    "\n",
    "# An inverted index is a map from terms to lists of documents that contain them.\n",
    "# Using set to prevent duplicates from when a word appears multiple times in a document.\n",
    "inverted_index = {term: set() for term in vocabulary}\n",
    "\n",
    "for doc_id, tokens in document_tokens.items():\n",
    "  for token in tokens:\n",
    "    inverted_index[token].add(doc_id)\n",
    "\n",
    "# Check result.\n",
    "for term in list(inverted_index.keys())[:3]:\n",
    "    print(term, \"→\", inverted_index[term])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogival → {359}\n",
      "disparity → {1359}\n",
      "alternate → {705, 618, 300}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "2b0e81bf",
   "metadata": {},
   "source": [
    "### Retrieve documents for a Boolean query (AND/OR)\n",
    "\n",
    "Create a function to retrieve documents for a Boolean query (AND/OR) with query terms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9c9318b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:17:32.482928Z",
     "start_time": "2025-09-09T09:17:32.480173Z"
    }
   },
   "source": [
    "# TODO: Create a function for retrieving documents for a Boolean query (AND/OR) with query terms\n",
    "\n",
    "def boolean_retrieve(query:str):\n",
    "# Your code here\n",
    "\n",
    "  query_parts = query.split()\n",
    "  result = inverted_index.get(query_parts[0], set())\n",
    "\n",
    "  index = 1\n",
    "  while index < len(query_parts):\n",
    "    operator = query_parts[index].upper()\n",
    "    term = query_parts[index + 1].lower()\n",
    "    docs = inverted_index.get(term, set())\n",
    "\n",
    "    if operator == \"AND\":\n",
    "      result = result & docs\n",
    "    elif operator == \"OR\":\n",
    "      result = result | docs\n",
    "    else:\n",
    "      raise ValueError(f\"Unknown operator: {operator}\")\n",
    "\n",
    "    index += 2\n",
    "\n",
    "  return sorted(list(result))\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "7bf47585",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T09:17:34.186580Z",
     "start_time": "2025-09-09T09:17:34.183986Z"
    }
   },
   "source": [
    "# Do not change this code\n",
    "boolean_queries = [\n",
    "  \"gas AND pressure\",\n",
    "  \"structural AND aeroelastic AND flight AND high AND speed OR aircraft\",\n",
    "  \"heat AND conduction AND composite AND slabs\",\n",
    "  \"boundary AND layer AND control\",\n",
    "  \"compressible AND flow AND nozzle\",\n",
    "  \"combustion AND chamber AND injection\",\n",
    "  \"laminar AND turbulent AND transition\",\n",
    "  \"fatigue AND crack AND growth\",\n",
    "  \"wing AND tip AND vortices\",\n",
    "  \"propulsion AND efficiency\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "eaf286d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:24:11.459151Z",
     "start_time": "2025-09-09T09:24:11.455335Z"
    }
   },
   "source": [
    "# Run Boolean queries in batch, using the function you created\n",
    "def run_batch_boolean(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = boolean_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "boolean_results = run_batch_boolean(boolean_queries)\n",
    "for qid, res in boolean_results.items():\n",
    "    print(qid, \"=>\", res[:10])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [27, 49, 85, 101, 110, 166, 168, 169, 183, 185]\n",
      "Q2 => [12, 14, 29, 47, 51, 75, 76, 78, 100, 172]\n",
      "Q3 => [5, 399]\n",
      "Q4 => [1, 61, 244, 265, 342, 416, 792, 798, 933, 974]\n",
      "Q5 => [118, 131]\n",
      "Q6 => []\n",
      "Q7 => [7, 9, 80, 89, 96, 142, 187, 207, 261, 294]\n",
      "Q8 => []\n",
      "Q9 => [675]\n",
      "Q10 => [968]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "4b591b81",
   "metadata": {},
   "source": [
    "## Part 1.2 – TF-IDF Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed448f",
   "metadata": {},
   "source": [
    "\n",
    "$tf_{i,j} = \\text{Raw Frequency}$ - The number of times a term appears in a document.\n",
    "\n",
    "$idf_t = \\log\\left(\\frac{N}{df_t}\\right)$ - N = total number of documents, df = number of documents that contain the term.\n",
    "\n",
    "### Build document–term matrix (TF and IDF weights)\n",
    "\n",
    "Compute tf and idf using the formulas above and store the weights in a document–term matrix (rows = documents, columns = terms).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "629e32fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:24:16.232580Z",
     "start_time": "2025-09-09T09:24:13.192398Z"
    }
   },
   "source": [
    "# TODO: Calculate the weights for the documents and the terms using tf and idf weighting. Put these values into a document–term matrix (rows = documents, columns = terms).\n",
    "\n",
    "# Your code here\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "N = len(documents) # Total number of documents.\n",
    "\n",
    "# Document frequency for each term.\n",
    "df = {term: len(doc_ids) for term, doc_ids in inverted_index.items()}\n",
    "\n",
    "# Inverted document frequency for each term.\n",
    "idf = {term: math.log(N / df) for term, df in df.items()}\n",
    "\n",
    "document_term_matrix = {}\n",
    "\n",
    "for doc_id, tokens in document_tokens.items():\n",
    "  tf_counter = Counter(tokens) # tf for each term in the current document.\n",
    "  document_term_matrix[doc_id] = {}\n",
    "  for term in vocabulary:\n",
    "    tf = tf_counter.get(term, 0) # tf for specific term.\n",
    "    document_term_matrix[doc_id][term] = tf * idf.get(term, 0) # TF * IDF\n",
    "\n",
    "# Convert dictionary to DataFrame to make it easier to work with.\n",
    "vocabulary_list = sorted(list(vocabulary)) # Columns do not support a set, need to convert the vocabulary to a list.\n",
    "dtm_df = pd.DataFrame.from_dict(document_term_matrix, orient='index', columns=vocabulary_list)\n",
    "\n",
    "print(dtm_df) # Check result.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ab  abbreviated  ability  ablated  ablating  ablation  ablative  able  \\\n",
      "1     0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "2     0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "3     0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "4     0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "5     0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "...   ...          ...      ...      ...       ...       ...       ...   ...   \n",
      "1396  0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "1397  0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "1398  0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "1399  0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "1400  0.0          0.0      0.0      0.0       0.0       0.0       0.0   0.0   \n",
      "\n",
      "      abrupt  abruptly  ...  zehnder      zero  zeros  zeroth  zhukhovitskii  \\\n",
      "1        0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "2        0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "3        0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "4        0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "5        0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "...      ...       ...  ...      ...       ...    ...     ...            ...   \n",
      "1396     0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "1397     0.0       0.0  ...      0.0  2.331573    0.0     0.0            0.0   \n",
      "1398     0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "1399     0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "1400     0.0       0.0  ...      0.0  0.000000    0.0     0.0            0.0   \n",
      "\n",
      "      zone  zones  zoom  zuk  zurich  \n",
      "1      0.0    0.0   0.0  0.0     0.0  \n",
      "2      0.0    0.0   0.0  0.0     0.0  \n",
      "3      0.0    0.0   0.0  0.0     0.0  \n",
      "4      0.0    0.0   0.0  0.0     0.0  \n",
      "5      0.0    0.0   0.0  0.0     0.0  \n",
      "...    ...    ...   ...  ...     ...  \n",
      "1396   0.0    0.0   0.0  0.0     0.0  \n",
      "1397   0.0    0.0   0.0  0.0     0.0  \n",
      "1398   0.0    0.0   0.0  0.0     0.0  \n",
      "1399   0.0    0.0   0.0  0.0     0.0  \n",
      "1400   0.0    0.0   0.0  0.0     0.0  \n",
      "\n",
      "[1400 rows x 6928 columns]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "6007cbf7",
   "metadata": {},
   "source": [
    "### Build TF–IDF document vectors\n",
    "\n",
    "From the matrix, build a TF–IDF vector for each document (consider normalization if needed for cosine similarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "654b0c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:24:31.346755Z",
     "start_time": "2025-09-09T09:24:31.292994Z"
    }
   },
   "source": [
    "\n",
    "# TODO: Build TF–IDF document vectors from the document–term matrix\n",
    "# Your code here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "document_vectors = {}\n",
    "\n",
    "for doc_id, row in dtm_df.iterrows():\n",
    "  vector = row.values.astype(float) # Convert each row in the document term matrix to a document vector.\n",
    "\n",
    "  magnitude = np.linalg.norm(vector)\n",
    "  if magnitude > 0:\n",
    "    vector = vector / magnitude # Normalize vector so that the length is 1.\n",
    "\n",
    "  document_vectors[doc_id] = vector\n",
    "\n",
    "# Check vectors.\n",
    "for doc_id in list(document_vectors.keys())[:3]:\n",
    "  print(f\"Doc {doc_id} vector: {document_vectors[doc_id][:20]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Doc 2 vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Doc 3 vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "36df320c",
   "metadata": {},
   "source": [
    "### Implement cosine similarity\n",
    "\n",
    "Implement a function to compute cosine similarity scores between a (tokenized) query and all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "44d2e7fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:32:33.965178Z",
     "start_time": "2025-09-09T09:32:33.961160Z"
    }
   },
   "source": [
    "\n",
    "# TODO: Create a function for calculating the similarity score of all the documents by their relevance to query terms\n",
    "\n",
    "def tfidf_retrieve(query: str):\n",
    "    # Your code here\n",
    "\n",
    "    query_parts = query.split()\n",
    "    tf_counter_query = Counter(query_parts) # tf for each term in the query.\n",
    "\n",
    "    # The vector must match the vocabulary size so query and document vectors are comparable.\n",
    "    query_vector = np.zeros(len(vocabulary))\n",
    "\n",
    "    for index, term in enumerate(vocabulary_list):\n",
    "      if term in tf_counter_query:\n",
    "        tf_query = tf_counter_query[term] # Term frequency for each term in the query.\n",
    "        query_vector[index] = tf_query * idf.get(term, 0)\n",
    "\n",
    "    # Normalize query vector.\n",
    "    magnitude_query = np.linalg.norm(query_vector)\n",
    "    if magnitude_query > 0:\n",
    "      query_vector = query_vector / magnitude_query\n",
    "\n",
    "    scores = {} # Values between 0 (no similarly) and 1 (identical content).\n",
    "    for doc_id, doc_vector in document_vectors.items():\n",
    "      similarity = float(np.dot(query_vector, doc_vector))\n",
    "      scores[doc_id] = similarity\n",
    "\n",
    "    ranked_documents = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_documents\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "d4968750",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T09:32:35.870855Z",
     "start_time": "2025-09-09T09:32:35.868173Z"
    }
   },
   "source": [
    "# Do not change this code\n",
    "tfidf_queries = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "18861681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:32:37.575317Z",
     "start_time": "2025-09-09T09:32:37.537160Z"
    }
   },
   "source": [
    "# Run TF-IDF queries in batch (print top-5 results for each), using the function you created\n",
    "def run_batch_tfidf(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = tfidf_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "tfidf_results = run_batch_tfidf(tfidf_queries)\n",
    "\n",
    "for qid, res in tfidf_results.items():\n",
    "    print(qid, \"=>\", res[:4])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [(169, 0.3264772671329021), (1286, 0.3008925433931835), (167, 0.298238680232968), (185, 0.2822065495985724)]\n",
      "Q2 => [(12, 0.5274627281621773), (51, 0.3411923675365786), (746, 0.29878572016098404), (875, 0.29208208108755074)]\n",
      "Q3 => [(399, 0.6253357148814741), (144, 0.46639427061347216), (485, 0.4441821036132606), (5, 0.406749496370075)]\n",
      "Q4 => [(368, 0.3952257052043), (748, 0.3746534145938327), (638, 0.35794322304922394), (451, 0.30243005168793896)]\n",
      "Q5 => [(389, 0.29052624820998024), (118, 0.27718780183272584), (1187, 0.2664773147364016), (172, 0.2603483717758538)]\n",
      "Q6 => [(974, 0.24580667067168463), (628, 0.23388314284392048), (397, 0.22916719468407404), (308, 0.22385350738346987)]\n",
      "Q7 => [(418, 0.5512607808825641), (1264, 0.4246955942843387), (315, 0.37108249990632514), (272, 0.3625940045253208)]\n",
      "Q8 => [(768, 0.40030390719716785), (726, 0.3739708023618002), (1196, 0.3737047186693344), (883, 0.32582652155412056)]\n",
      "Q9 => [(1284, 0.35384882554362423), (433, 0.3097586233179562), (675, 0.26198985420552573), (1271, 0.2523095247716536)]\n",
      "Q10 => [(968, 0.4039275176210989), (1328, 0.2923302674826781), (1380, 0.19015304075061887), (1092, 0.1382673231684003)]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "e0989101",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1.3 – Conceptual Questions\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "**1. What is the difference between data retrieval and information retrieval?**\n",
    "\n",
    "*Your answer here*:\n",
    "\n",
    "The difference lies in the type of data and the retrieval method. Data retrieval deals with structured data and returns specific records based on structured queries. Information retrieval, on the other hand, deals with unstructured or semi-structured data, such as text documents, and returns the most relevant resources rather than exact matches, often based on natural language.\n",
    "\n",
    "**For the following scenarios, which approach would be suitable data retrieval or information retrieval? Explain your reasoning.** <br>\n",
    "1.a A clerk in pharmacy uses the following query: Medicine_name = Ibuprofen_400mg\n",
    "\n",
    "*Your answer here*:\n",
    "\n",
    "In this scenario, it would be most suitable to use data retrieval, because the query specifies an exact value for a predefined field, indicating that the user is looking for an exact match for a specific medicine.\n",
    "\n",
    "1.b A clerk in pharmacy uses the following query: An anti-biotic medicine \n",
    "\n",
    "*Your answer here*:\n",
    "\n",
    "In this scenario, it would be most suitable to use information retrieval, because the query is in natural language and is vague. The system should return all relevant medicines matching the description rather than an exact match.\n",
    "\n",
    "1.c Searching for the schedule of a flight using the following query: Flight_ID = ZEFV2\n",
    "\n",
    "*Your answer here*:\n",
    "\n",
    "In this scenario, it would be most suitable to use data retrieval, because the query specifies a unique identifier for an exact match.\n",
    "\n",
    "1.d Searching an E-commerce website using the following query to find a specific shoe: Brooks Ghost 15\n",
    "\n",
    "*Your answer here*:\n",
    "\n",
    "In this scenario, it would be most suitable to use information retrieval, because the search may need to rank results by relevance and handle different variations of the product.\n",
    "\n",
    "1.e Searching the same E-commerce website using the following query: Nice running shoes\n",
    "\n",
    "*Your answer here*:\n",
    "\n",
    "In this scenario, it would be most suitable to use information retrieval, because the query is vague and in natural language, requiring the system to identify and rank products that match the description rather than an exact match.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
